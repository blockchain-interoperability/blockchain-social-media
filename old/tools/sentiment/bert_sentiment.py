# -*- coding: utf-8 -*-
"""BERT_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBQqFUJwsK4rw3t1yBNX3WTu7CJ74VNS
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nna
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

"""
This section grabs the SemEval dataset from Dropbox.
Requires a dropbox API key.
"""
import os.path
import dropbox

"""
This section grabs the SemEval dataset from Dropbox.
Requires a dropbox API key.
"""
def downloadDataset(fileurl: str, outpath: str='semeval.zip'):
    if '.zip' not in outpath:
        outpath = f'{outpath}.zip'

    if not os.path.isfile(outpath):
      res = requests.get(fileurl, allow_redirects=True)
      with open(outpath, "wb") as f:
          f.write(res.content)

"""
Create a generator for SemEval Twitter Data
The data files are separated by tab and each folder has it's own data format
"""
def parseTwitter(folders):
  mapping = {'-2': 'negative', '-1': 'negative', '0': 'neutral', '1': 'positive', '2': 'positive'}

  for folder in folders:
    for file in [f'{folder}/{f}' for f in os.listdir(folder) if 'twitter' in f]:
      with open(file, 'r') as f:
        for line in f:
          segments = line.rstrip().split('\t')
          if len(segments) < 3:
            continue
          elif 'A' in folder:
            tweet_data = [segments[0], segments[1], ''.join(segments[2:])]
          elif 'B' in folder:
            tweet_data = [segments[0], segments[-2], segments[-1]]
          else:
            tweet_data = [segments[0], mapping[segments[-2]], segments[-1]]
          yield tweet_data

"""
This section processes the data files and loads the data into a dataframe
"""
def loadData(datapath: str='semeval'):
    # unzip data if folder is not present
    if not os.path.isdir(datapath):
      import zipfile
      with zipfile.ZipFile(f'{datapath}.zip', 'r') as zip_ref:
          zip_ref.extractall(datapath)

    # get all folders with relavent data in them
    task_folders = [f'{datapath}/2017_English_final/GOLD/{folder}'
                    for folder in os.listdir(f'{datapath}/2017_English_final/GOLD') 
                    if os.path.isdir(f'{datapath}/2017_English_final/GOLD/{folder}')
    ]

    # create a pandas dataframe
    return pd.DataFrame(parseTwitter(task_folders), columns=['TweetId', 'Sentiment', 'Text'])

downloadDataset("https://www.dropbox.com/s/byzr8yoda6bua1b/2017_English_final.zip?dl=1")
df = loadData()

# f1 score and compute balanced accuracy
df.groupby(['Sentiment']).agg('count')

df.head()

"""
Clean up data and
Split the input into training set and test set
The data is split as follows
70 % training set, 15% test set, 15% validation set
"""
# eliminate off topic tweets and make categorical
df = df[df.Sentiment != 'off topic']
df.Sentiment = pd.Categorical(df.Sentiment)
df.Sentiment = df.Sentiment.map({'negative': 0, 'neutral': 1, 'positive':2})

train_text, temp_text, train_labels, temp_labels = train_test_split(df['Text'], df['Sentiment'], 
                                                                    random_state=2018, 
                                                                    test_size=0.3, 
                                                                    stratify=df['Sentiment'])

val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, 
                                                                random_state=2018, 
                                                                test_size=0.5, 
                                                                stratify=temp_labels)

"""
Install CT-Bert from huggingface along with other modules
"""
from transformers import (
   AutoConfig,
   AutoTokenizer,
   AutoModelForSequenceClassification,
   AdamW
)

"""
Choose the tokenizer from CT-BERT
use v2
"""
model_name = 'digitalepidemiologylab/covid-twitter-bert-v2'
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""
Use CT-BERT to Tokenize the tweet text for training, test and validation sets
uses the latest API provided by huggingface for text encoding
"""
max_seq_len = 35
train_encodings = tokenizer(
    train_text.tolist(),
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=max_seq_len,
)

val_encodings = tokenizer(
    val_text.tolist(),
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=max_seq_len
)

test_encodings = tokenizer(
    test_text.tolist(),
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=max_seq_len,
)

"""
Use pytorch dataloader to ingest the data and convert to tensors
"""
class SemEvalDataset(torch.utils.data.Dataset):
  def __init__(self, encodings, labels):
    self.encodings = encodings
    self.labels = labels
  
  def __getitem__(self, idx):
    item = {key: val[idx] for key, val in self.encodings.items()}
    item['labels'] = torch.tensor(self.labels.iloc[idx])
    return item

  def __len__(self):
    return len(self.labels)

train_dataset = SemEvalDataset(train_encodings, train_labels)
val_dataset = SemEvalDataset(val_encodings, val_labels)
test_dataset = SemEvalDataset(test_encodings, test_labels)

import time
import datetime
def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

"""
Training loop to finetune CT Bert to classify sentiment
"""

# declare a model, export to GPU if available and set to training
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = AutoModelForSequenceClassification.from_pretrained(
    "digitalepidemiologylab/covid-twitter-bert-v2",
    num_labels = 3
)

model.to(device)
model.train()

# create data loaders based off of training and validation set
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)

accuracies, training_losses, val_losses = [], [], []

optim = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
epochs = 25

# main training loop
for epoch in range(epochs):
  print("")
  print(f'======== Epoch {epoch + 1} / {epochs} ========')
  print('Training...')
  total_train_loss = 0
  t0 = time.time()
  model.train()
    
  # loop through all batches in data loader
  for step, batch in enumerate(train_loader):
    if step % 40 == 0 and not step == 0:
      # Calculate elapsed time in minutes.
      elapsed = format_time(time.time() - t0)
            
      # Report progress.
      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))

    # clear gradient from previous iteration
    optim.zero_grad()
    model.zero_grad()

    # send input encodings, attention mask and labels to GPU
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    # feed the batch of data into the model
    loss, logits = model(input_ids, attention_mask=attention_mask, labels=labels)
    total_train_loss += loss.item()

    # clip the norm of the gradients to 1
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    loss.backward()
    optim.step()
    
  # calculate average loss
  avg_train_loss = total_train_loss / len(train_loader)
  training_losses.append(avg_train_loss)
  # Measure how long this epoch took.
  training_time = format_time(time.time() - t0)
  print("  Average training loss: {0:.2f}".format(avg_train_loss))
  print("  Training epcoh took: {:}".format(training_time))

  print("Running Validation...")
  t0 = time.time()
  model.eval()

  # Tracking variables 
  total_eval_accuracy = 0
  total_eval_loss = 0
  nb_eval_steps = 0
  
  # run performance analysis on validation dataset
  for batch in val_loader:
    # send batch values to gpu
    b_input_ids = batch['input_ids'].to(device)
    b_input_mask = batch['attention_mask'].to(device)
    b_labels = batch['labels'].to(device)

    # feed tata into model without training
    with torch.no_grad():
      loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
    total_eval_loss += loss.item()
    
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()

    # calculate accuracy on the validation dataset
    pred_flat = np.argmax(logits, axis=1).flatten()
    labels_flat = label_ids.flatten()
    total_eval_accuracy += np.sum(pred_flat == labels_flat) / len(labels_flat)
  
  # Report the final accuracy for this validation run.
  avg_val_accuracy = total_eval_accuracy / len(val_loader)
  accuracies.append(avg_val_accuracy)
  print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

  # Calculate the average loss over all of the batches.
  avg_val_loss = total_eval_loss / len(val_loader)
  val_losses.append(avg_val_loss)
    
  # Measure how long the validation run took.
  validation_time = format_time(time.time() - t0)
    
  print("  Validation Loss: {0:.2f}".format(avg_val_loss))
  print("  Validation took: {:}".format(validation_time))

torch.save(model, './sentiment.pt')
with open('training_data.pickle', 'wb') as f:
  pickle.dump({'accuracy': accuracies, 'training_loss': training_losses, 'val_loss': val_losses}, f)


"""
upload model to dropbox in case runtime disconects
"""

def largeUpload(file_path, dest_path):
  with open(file_path, 'rb') as f:
    file_size = os.path.getsize(file_path)
    CHUNK_SIZE = 4 * 1024 * 1024
    upload_session_start_result = dbx.files_upload_session_start(f.read(CHUNK_SIZE))
    cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id, offset=f.tell())
    commit = dropbox.files.CommitInfo(path=dest_path)

    while f.tell() < file_size:
      if ((file_size - f.tell()) <= CHUNK_SIZE):
        print(dbx.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit))
      else:
        dbx.files_upload_session_append(f.read(CHUNK_SIZE), cursor.session_id, cursor.offset)
        cursor.offset = f.tell()
# largeUpload('./sentiment.pt', '/sentiment.pt')

"""
Load saved model
"""
if not model or not os.path.isfile('./sentiment.pt'):
  with open("sentiment.pt", "wb") as f:
    metadata, res = dbx.files_download(path="/sentiment.pt")
    f.write(res.content)
  model = torch.load('./sentiment.pt')

"""
See how well the model performs on the testing dataset
"""
model.eval()

# Tracking variables 
predictions , actual = np.array([]), np.array([])
loss_data = np.array([])
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)

# Predict 
print("Running Testing...")
t0 = time.time()
  
# run performance analysis on testing dataset
for batch in test_loader:
  # send batch values to gpu
  b_input_ids = batch['input_ids'].to(device)
  b_input_mask = batch['attention_mask'].to(device)
  b_labels = batch['labels'].to(device)

  with torch.no_grad():
    loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
  loss_data = np.append(loss_data, loss.item())
    
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  # calculate accuracy on the testing dataset
  pred_flat = np.argmax(logits, axis=1).flatten()
  labels_flat = label_ids.flatten()
  predictions = np.append(predictions, pred_flat)
  actual = np. append(actual, labels_flat)
      
# Measure how long the validation run took.
testing_time = format_time(time.time() - t0)

print("  Testing took: {:}".format(testing_time))

# make stuff human readable
lookup = {0: 'negative', 1: 'neutral', 2: 'positive'}
hactual = np.array(list(map(lambda x: lookup[int(x)], actual)))
hpred = np.array(list(map(lambda x: lookup[int(x)], predictions)))

# calculate accuracy
test_acc = np.sum(hactual == hpred) / len(actual)
print("Accuracy: {0:.2f}".format(test_acc))

# plot confusion matrix
y_actu = pd.Series(hactual, name='Actual')
y_pred = pd.Series(hpred, name='Predicted')
df_confusion = pd.crosstab(y_actu, y_pred)

print(df_confusion)


# plot metrics on the data
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_actu, y_pred)

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))
